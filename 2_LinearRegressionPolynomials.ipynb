{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression for polynomials\n",
    "\n",
    "In this notebook, you will perform linear regression to predict the values of function for which you have the values at a certain number of points. The aim to familiarize yourself with the concepts of underfitting and overfitting, and also with the impact of penalization strategies such as ridge regression and LASSO regularization.\n",
    "\n",
    "**There are 10 questions to answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data creation\n",
    "\n",
    "The data is created by perturbing the output of a given polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacing between test points\n",
    "dx_test = 0.1\n",
    "\n",
    "# noise level for perturbation\n",
    "sigma2 = 20\n",
    "\n",
    "# function to learn\n",
    "w = np.array([-1.5, 1 / 9.0])\n",
    "def fct_to_learn(x): \n",
    "    return w[0] * x + w[1] * np.square(x)\n",
    "    \n",
    "# the parameter n is the number of train data points that will be considered\n",
    "def make_1dregression_data(n=21):\n",
    "    # reset the random seed when recreating data\n",
    "    np.random.seed(0)\n",
    "    # creation of x values for train and test; fixed design\n",
    "    xtrain = np.linspace(0.0, 20, n)\n",
    "    xtest = np.arange(0.0, 20, dx_test)\n",
    "    # associated y values: function defined above, perturbed by noise \n",
    "    ytrain = fct_to_learn(xtrain) + np.random.normal(0, 1, xtrain.shape) * np.sqrt(sigma2)\n",
    "    ytest = fct_to_learn(xtest) + np.random.normal(0, 1, xtest.shape) * np.sqrt(sigma2)\n",
    "    return xtrain, ytrain, xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 51\n",
    "xtrain, ytrain, xtest, ytest = make_1dregression_data(n_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the data to visualize the learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(xtest,ytest,color='red',s=5)\n",
    "plt.scatter(xtrain,ytrain,color='black')\n",
    "plt.legend(['test','train'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** How would you change the code in order to do random design, *i.e.* consider training points sampled uniformly in the interval $[0,20]$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "xtrain = np.random.uniform(0.0,20,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next rescale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before rescaling\n",
    "print('Before rescaling')\n",
    "print('Train data: min =',np.min(xtrain),'max =',np.max(xtrain))\n",
    "print('Test data:  min =',np.min(xtest), 'max =',np.max(xtest))\n",
    "\n",
    "# Rescaling data, based on the train data + reshaping as column vector\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "Xtrain = scaler.fit_transform(xtrain.reshape(-1, 1))\n",
    "Ytrain = ytrain.reshape(-1, 1)\n",
    "Xtest = scaler.transform(xtest.reshape(-1, 1))\n",
    "Ytest = ytest.reshape(-1, 1)\n",
    "\n",
    "# After rescaling\n",
    "print('\\nAfter rescaling (based on train data)')\n",
    "print('Train data: min =',np.min(Xtrain),'max =',np.max(Xtrain))\n",
    "print('Test data:  min =',np.min(Xtest), 'max =',np.max(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and predicting with polynomials of various degrees\n",
    "\n",
    "We next consider fitting the model with polynomials of increasing degrees, and compute the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_deg = 10\n",
    "degree = np.arange(0,max_deg+1,1)\n",
    "features = Xtrain**degree\n",
    "print(\"Shape of the feature vector:\",features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Complete the code below to compute the weight obtained by ordinary least-squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = []\n",
    "\n",
    "for k in degree:\n",
    "    # extract the first k+1 columns (i.e. 1, x, x**2, ..., x**k)\n",
    "    feat = features[:,0:(k+1)]\n",
    "    # compute the matrix (X^T X)^{-1} X^T\n",
    "    # use @ for matrix multiplication [see https://peps.python.org/pep-0465/] \n",
    "    # and .T for transpose [see https://numpy.org/doc/stable/reference/generated/numpy.ndarray.T.html] \n",
    "    mat = np.linalg.inv( (feat.T)@feat )@(feat.T)\n",
    "    # obtain the weights (denoted by theta in the lecture)\n",
    "    weights = mat@Ytrain\n",
    "    # append the weights for polynomials of degree k to the list of weights\n",
    "    coeff.append(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now the various polynomial predictions, as a function of the degree of the polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we compute the design matrix for the test data\n",
    "test_features = Xtest**degree\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "# looping over the polynomial degrees\n",
    "for k in degree:\n",
    "    # we compute the predictions for each test value x\n",
    "    values = test_features[:,0:(k+1)]@coeff[k]\n",
    "    # we plot these values as a function of the test values x\n",
    "    plt.plot(xtest,values,label=k)\n",
    "plt.legend()\n",
    "plt.scatter(xtrain,ytrain,color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.** What happens in the absence of noise? Are the coefficients of the predicted polynomials close to the initial coefficients of the function *fct_to_learn*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a perfect fit when there is no noise. Note that the coefficients are different from the original coefficients in the unknown function since the data has been rescaled on the $x$ axis. When noise is present, we can compare the performance of the fit on the train and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can next plot the train and test errors as a function of the complexity of the model (degree of the polynomial here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = np.zeros(degree.shape)\n",
    "test_loss = np.zeros(degree.shape)\n",
    "\n",
    "test_features = Xtest**degree\n",
    "train_features = Xtrain**degree\n",
    "\n",
    "n_train = Xtrain.shape[0]\n",
    "n_test = Xtest.shape[0]\n",
    "\n",
    "for k in degree:\n",
    "    train_values = train_features[:,0:(k+1)]@coeff[k]\n",
    "    train_loss[k] = np.sum((train_values-Ytrain)**2)/n_train\n",
    "    test_values = test_features[:,0:(k+1)]@coeff[k]\n",
    "    test_loss[k] = np.sum((test_values-Ytest)**2)/n_test\n",
    "    \n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(degree,train_loss,color='blue',label='train loss')\n",
    "plt.scatter(degree,test_loss,color='red',label='test loss')\n",
    "plt.plot(degree,sigma2*np.ones(degree.shape),color='black',label='noise floor')\n",
    "plt.xlabel('degree of the polynomial')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.** In view of the results, which value of the degree of the polynomial should be chosen? How does the predictions behave when the number of training data is varied? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The behavior of the test error suggests that a polynomial of degree 2 should be used. Note also that there is a lower bound to the MSE, which depends on the level of noise (the so-called \"noise floor\" mentioned in Section 4.5.7 of [Murphy]).\n",
    "\n",
    "When more data is added, the test error comes closer to the train error. Note that for large enough degrees and when there is sufficient data, the train error is typically below the noise floor, while the test error is above. Both converge the noise floor as the sample size is increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.** Resume the study for randomly spaced input points $x_i$ (\"random design\" of Question 1). What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors are larger with randomly spaced input points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization with ridge regression from sratch\n",
    "\n",
    "We now fix the degree of the polynomial to a large value, and add a regularization term to the loss function in order to prevent overfitting. The magnitude of the regularization is determined by cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = 15\n",
    "degree = np.arange(0,deg+1,1)\n",
    "train_features = Xtrain**degree\n",
    "test_features = Xtest**degree\n",
    "n_train = Xtrain.shape[0]\n",
    "n_test = Xtest.shape[0]\n",
    "print(\"Shape of the feature vector (train):\",train_features.shape)\n",
    "\n",
    "# we choose values of the regularization parameter logarithmically spaced\n",
    "lambdas = np.logspace(-10, 2, 50)\n",
    "n_lambdas = len(lambdas)\n",
    "train_loss = np.empty(n_lambdas)\n",
    "test_loss = np.empty(n_lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6.** Implement the predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we iterate over the values of alpha and compute the corresponding predictions\n",
    "for k, alpha in enumerate(lambdas):\n",
    "    # COMPUTE THE MATRIX FOR RIDGE REGRESSION; mind the n_train factor...\n",
    "    mat = np.linalg.inv( (train_features.T)@train_features + n_train*alpha*np.identity(deg+1))@(train_features.T)\n",
    "    weights = mat@Ytrain\n",
    "    train_values = train_features@weights\n",
    "    train_loss[k] = np.sum((train_values-Ytrain)**2)/n_train\n",
    "    test_values = test_features@weights\n",
    "    test_loss[k] = np.sum((test_values-Ytest)**2)/n_test\n",
    "    \n",
    "# we can finally plot the results\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.semilogx(lambdas,train_loss,color='blue',label='train loss',marker='o')\n",
    "plt.semilogx(lambdas,test_loss,color='red',label='test loss',marker='o')\n",
    "plt.xlabel('value of the regularization parameter')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7.** Determine the optimal regularization parameter $\\lambda_n^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.argmin(test_loss)\n",
    "alpha = lambdas[index]\n",
    "print(\"Value of the optimal regularization parameter\",alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally plot the best prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth corresponding to noiseless data\n",
    "reference_values = fct_to_learn(xtest)\n",
    "\n",
    "# best prediction with ridge regularization\n",
    "mat = np.linalg.inv( (train_features.T)@train_features + n_train*alpha*np.identity(deg+1))@(train_features.T)\n",
    "weights = mat@Ytrain\n",
    "test_values = test_features@weights\n",
    "\n",
    "# plotting the results\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(Xtrain, Ytrain)\n",
    "plt.plot(Xtest, test_values,color='red',label='prediction')\n",
    "plt.plot(Xtest, reference_values,color='black',label='reference')\n",
    "plt.title(\"Ridge regularization with lambda = {:0.5f}\".format(alpha))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also plot some learning curves, as done in Section 4.5.7 of [Murphy] for instance.\n",
    "\n",
    "**Question 8.** How does the value of $\\lambda^*$ depend on the number of data points $n$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $n=21$, one finds $\\lambda^* = 0.021$. For $n=51$ and $101$, one finds $\\lambda^* = 0.012$. For $n=201$, one has $\\lambda^* = 0.007$. The value decreases when the sample size increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization with ridge regression using scikit-learn\n",
    "\n",
    "We finally use the built-in functions of scikit-learn to reproduce the above results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = np.empty(n_lambdas)\n",
    "test_loss = np.empty(n_lambdas)\n",
    "ytest_pred_stored = dict()\n",
    "\n",
    "for i, alpha in enumerate(lambdas):\n",
    "    # choose the model: ridge regression\n",
    "    # see https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "    model = Ridge(alpha=alpha, fit_intercept=True)\n",
    "    # preprocessing: construct the feature vector, using functions specific for polynomial fitting\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\n",
    "    poly_features = PolynomialFeatures(degree=deg, include_bias=True)\n",
    "    Xtrain_poly = poly_features.fit_transform(Xtrain)\n",
    "    # fit the model\n",
    "    model.fit(Xtrain_poly, ytrain)\n",
    "    # predict the output for the train and test data\n",
    "    ytrain_pred = model.predict(Xtrain_poly)\n",
    "    train_loss[i] = mse(ytrain_pred, ytrain)\n",
    "    Xtest_poly = poly_features.transform(Xtest)\n",
    "    ytest_pred = model.predict(Xtest_poly)\n",
    "    test_loss[i] = mse(ytest_pred, ytest)\n",
    "    # store the prediction on the test data for further use\n",
    "    ytest_pred_stored[alpha] = ytest_pred\n",
    "    \n",
    "plt.figure(figsize=(8,8))\n",
    "plt.semilogx(lambdas,train_loss,color='blue',label='train loss',marker='o')\n",
    "plt.semilogx(lambdas,test_loss,color='red',label='test loss',marker='o')\n",
    "plt.xlabel('value of the regularization parameter')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9.** Compare the optimal value with the one found above. How do they compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.argmin(test_loss)\n",
    "alpha = lambdas[index]\n",
    "print(\"Value of the optimal regularization parameter\",alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values are quite different since the normalizations in the functions which are optimized are different. Indeed, as mentioned in https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html, the function which is optimized is $\\|Y-X\\theta\\|^2 + \\lambda \\|\\theta\\|^2$ and not $\\frac1n \\|Y-X\\theta\\|^2 + \\lambda \\|\\theta\\|^2$ as we considered. The value returned by sklearn is therefore (close to) the value we have computed multiplied by the sample size $n$; see the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha/n_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot a few selected predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "max_index_lambda = len(lambdas)\n",
    "chosen_lambdas = lambdas[[0, int(max_index_lambda/3), int(2*max_index_lambda/3), max_index_lambda-1]]\n",
    "for i, alpha in enumerate(chosen_lambdas):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.scatter(Xtrain, ytrain)\n",
    "    plt.plot(Xtest, ytest_pred_stored[alpha])\n",
    "    plt.title(\"L2 regularizer {:0.10f}\".format(alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization with LASSO \n",
    "\n",
    "We finally consider LASSO instead of ridge regression. When the design matrix $X$ is such that $X^\\top X = n \\mathrm{Id}$, the solution to Lasso can be computed explicitly, see for instance pages 19 and 24 of http://weixingsong.weebly.com/uploads/7/4/3/5/7435707/stat905_lecture5.pdf\n",
    "In the general case, finding the weights should be seen as a convex programming problem, solved here using built-in scikit-learn functions, which perform coordinate descent (as solving one dimensional problems in Lasso is simple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we choose values of the regularization parameter logarithmically spaced\n",
    "lambdas = np.logspace(-5, 1, 250)\n",
    "n_lambdas = len(lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = np.empty(n_lambdas)\n",
    "test_loss = np.empty(n_lambdas)\n",
    "ytest_pred_stored = dict()\n",
    "\n",
    "for i, alpha in enumerate(lambdas):\n",
    "    # choose the model: LASSO regression\n",
    "    # see https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "    model = Lasso(alpha=alpha, fit_intercept=True)\n",
    "    # preprocessing (as for ridge regression)\n",
    "    poly_features = PolynomialFeatures(degree=deg, include_bias=True)\n",
    "    Xtrain_poly = poly_features.fit_transform(Xtrain)\n",
    "    # fit the model\n",
    "    model.fit(Xtrain_poly, ytrain)\n",
    "    # predict the output for the train and test data\n",
    "    ytrain_pred = model.predict(Xtrain_poly)\n",
    "    train_loss[i] = mse(ytrain_pred, ytrain)\n",
    "    Xtest_poly = poly_features.transform(Xtest)\n",
    "    ytest_pred = model.predict(Xtest_poly)\n",
    "    test_loss[i] = mse(ytest_pred, ytest)\n",
    "    # store the prediction on the test data for further use\n",
    "    ytest_pred_stored[alpha] = ytest_pred\n",
    "    \n",
    "plt.figure(figsize=(8,8))\n",
    "plt.semilogx(lambdas,train_loss,color='blue',label='train loss',marker='o')\n",
    "plt.semilogx(lambdas,test_loss,color='red',label='test loss',marker='o')\n",
    "plt.xlabel('value of the regularization parameter')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the numerical method to perform optimization has troubles converging for small values of $\\lambda$... Let's investigate this more precisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_value = 0.001\n",
    "model = Lasso(alpha=alpha_value, fit_intercept=True)\n",
    "poly_features = PolynomialFeatures(degree=deg, include_bias=True)\n",
    "Xtrain_poly = poly_features.fit_transform(Xtrain)\n",
    "model.fit(Xtrain_poly, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to fix the value of $\\lambda$ and already have rescaled features, the only remaining option is to increase the number of iterations (set by default to 1000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_value = 0.001\n",
    "# increase progressively from 1000, as 2000, 3000, ... to see the better convergence \n",
    "# (measured in terms of duality gap/tolerance)\n",
    "maximal_nb_iterations = 1000 \n",
    "model = Lasso(alpha=alpha_value, fit_intercept=True, max_iter=maximal_nb_iterations)\n",
    "poly_features = PolynomialFeatures(degree=deg, include_bias=True)\n",
    "Xtrain_poly = poly_features.fit_transform(Xtrain)\n",
    "model.fit(Xtrain_poly, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We come back to finding the best value of the regularization parameter for LASSO, and looking at the associated predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.argmin(test_loss)\n",
    "alpha_opt = lambdas[index]\n",
    "print(\"Value of the optimal regularization parameter:\",alpha_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(Xtrain, Ytrain)\n",
    "plt.plot(Xtest,ytest_pred_stored[alpha_opt],color='red',label='prediction')\n",
    "plt.plot(Xtest,reference_values,color='black',label='reference')\n",
    "plt.title(\"Lasso regularization with lambda = {:0.5f}\".format(alpha_opt))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10.** Determine the coefficients obtained with LASSO for the optimal value of the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(alpha_opt, fit_intercept=True)\n",
    "model.fit(Xtrain_poly, ytrain)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many coefficients are set to 0! One sees very well the sparsity inducing effect of the regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
