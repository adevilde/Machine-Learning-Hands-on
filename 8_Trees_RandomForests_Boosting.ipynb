{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trees, random forests, and boosting\n",
    "\n",
    "We investigate in this notebook how methods based on trees perform for a regression problem (on the 'bike sharing' data set already encountered in the notebook on linear regression).\n",
    "\n",
    "**There are 6 questions to answer. Only the part on trees is due for Apr. 3rd, 2024 (the part on boosting will be for Apr. 8th).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# set up the random number generator: given seed for reproducibility, None otherwise\n",
    "# (see https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.default_rng)\n",
    "my_seed = 1\n",
    "rng = np.random.default_rng(seed=my_seed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We re-use the 'bike sharing' data set, used for linear regression. We therefore do not further comment this example, and directly build the test and train sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('BikeSharingDataset.csv')\n",
    "df.drop(['instant','dteday','temp','casual','registered'],axis=1,inplace=True)\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly splitting between test and train data\n",
    "fraction_train = 0.7 \n",
    "fraction_test = 1.0 - fraction_train\n",
    "df_train, df_test = train_test_split(df, train_size = fraction_train, test_size = fraction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the aim is to predict the last column from the previous ones\n",
    "X_train = df_train[df_train.columns[:-1]]\n",
    "y_train = df_train[\"cnt\"]\n",
    "print(\"Train set\")\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construction of the test set\n",
    "X_test = df_test[df_test.columns[:-1]]\n",
    "y_test = df_test[\"cnt\"]\n",
    "print(\"Test set\")\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees\n",
    "\n",
    "We start by performing regression with a single tree. See https://scikit-learn.org/stable/modules/tree.html for a presentation of trees in Scikit-learn (in particular practical tips in Section 1.10.5, including elements on how to avoid overfitting), as well as https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html for a description of the various functions in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key parameters to avoid overfitting are\n",
    "- *max_depth* to limit the maximal depth of the tree\n",
    "- *min_samples_leaf* to ask for a minimal number of data points for a condition to be satisfied\n",
    "We start by playing around with a few values of these parameters to get a feeling of what is done/obtained.\n",
    "\n",
    "**Question 1.** Complete the code below to perform the desired prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choice of parameters \n",
    "chosen_max_depth = ... # TO COMPLETE\n",
    "chosen_min_samples_leaf = ... # TO COMPLETE\n",
    "\n",
    "# constructing + fitting the model and making the prediction\n",
    "dt = DecisionTreeRegressor(...) # TO COMPLETE WITH CORRECT ARGUMENTS \n",
    "model = dt.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# computing the performance on the test set\n",
    "# MSE between prediction and true values\n",
    "mse = mean_squared_error(y_pred,y_test)\n",
    "print('MSE (test):',mse) \n",
    "# coefficient of determination for the regression\n",
    "R2 = dt.score(X_test,y_test)\n",
    "print('R2  (test):',R2)\n",
    "# plotting predicted values as a function of true values to graphically assess the quality of regression\n",
    "plt.title('Correlation between actual and predicted values')\n",
    "plt.xlabel('actual values')\n",
    "plt.ylabel('predicted values')\n",
    "plt.scatter(y_test,y_pred,color='royalblue')\n",
    "plt.plot([0,1],[0,1],color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the tree that was learnt using the function **plot_tree** (see https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "print('Number of data points : ',y_train.shape[0])\n",
    "for pair in zip(df_train.columns, np.arange(df_train.shape[0])):\n",
    "  print('X[',pair[1],'] = ', pair[0]) \n",
    "plt.figure(figsize=(15, 10))\n",
    "tree.plot_tree(dt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** For a minimal number of samples at leaves of 10, what is the best tree depth that you find, and the associated errors? What do you think of these results? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WRITE YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next explore more systematically the choice of the parameters by cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor()\n",
    "dt_params = {'max_depth':np.arange(1,10),'min_samples_leaf':np.arange(5,20)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.** Complete the code below to find the best hyperparameters for the model and then retrain the corresponding model and compute the associated outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output (best parameters) is in the 'dict' format = dictionary\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "print('Grid search to find optimal parameters')\n",
    "gs_dt = GridSearchCV(...) # TO COMPLETE\n",
    "gs_dt.fit(X_train,y_train)\n",
    "a = ... # BEST PARAMETERS, see attributes of grid search class\n",
    "print('- Best maximal depth =',...)\n",
    "print('- Best minimal number of samples in the leaves = ',...,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training with best parameters\n",
    "dt_final = DecisionTreeRegressor(...) # TO COMPLETE\n",
    "model = dt_final.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# computing the performance on the test set\n",
    "# MSE between prediction and true values\n",
    "mse = mean_squared_error(y_pred,y_test)\n",
    "print('MSE (test):',mse) \n",
    "# coefficient of determination for the regression\n",
    "R2 = dt_final.score(X_test,y_test)\n",
    "print('R2  (test):',R2)\n",
    "# plotting predicted values as a function of true values to graphically assess the quality of regression\n",
    "plt.title('Correlation between actual and predicted values')\n",
    "plt.xlabel('actual values')\n",
    "plt.ylabel('predicted values')\n",
    "plt.scatter(y_test,y_pred,color='royalblue')\n",
    "plt.plot([0,1],[0,1],color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.** Is the value better than the one found manually above? Do the values found by cross validation change a lot if one does another test/train split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WRITE YOUR ANSWERE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forests\n",
    "\n",
    "The test error of decision trees alone is not good. It is much better to combine regression trees into random forest (see https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here as well, we will find the best parameters for *RandomForestRegressor* by cross validation. The key parameters to set are \n",
    "- *n_estimators* which is the number of trees in the forest\n",
    "- *max_depth* and *min_sample_leaf* which are the same parameters as for decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.** Fit the best model with the hyperparameters found by cross validation, and compare its predictive performance to the one found with a single tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {'n_estimators':np.arange(25,150,25),'max_depth':np.arange(1,11,2),'min_samples_leaf':np.arange(2,15,3)}\n",
    "print('Grid search to find optimal parameters')\n",
    "gs_rf = GridSearchCV(...) # TO COMPLETE\n",
    "gs_rf.fit(X_train,y_train)\n",
    "b = ... # TO COMPLETE, best parameters\n",
    "print('- Best number of trees = ',...)\n",
    "print('- Best maximal depth =',...)\n",
    "print('- Best minimal number of samples in the leaves = ',...,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model with best params\n",
    "RF = RandomForestRegressor(n_estimators=...,max_depth=...,min_samples_leaf=...) # TO COMPLETE\n",
    "model = RF.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the performance on the test set\n",
    "R2 = RF.score(X_test,y_test)\n",
    "print('R2  (test):',R2)\n",
    "mse = mean_squared_error(y_pred,y_test)\n",
    "print('MSE (test):',mse) \n",
    "plt.title('Correlation between actual and predicted values')\n",
    "plt.xlabel('actual values')\n",
    "plt.ylabel('predicted values')\n",
    "plt.scatter(y_test,y_pred,color='royalblue')\n",
    "plt.plot([0,1],[0,1],color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WRITE YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "We can also consider boosting methods based on trees (see https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html and the example in https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_regression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "ar = AdaBoostRegressor(base_estimator = dt_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the best parameters for AdaBoostRegressor for base learners corresponding to on the decision tree found in Question 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6.** Fit the best model with the hyperparameters found by grid search, and compare its predictive performance to the one found with a single tree and random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tree maximal depth (weak learner) =',dt_final.max_depth)\n",
    "print('Minimal number of samples in the leaves (weak learner) = ',dt_final.min_samples_leaf,'\\n')\n",
    "# key parameter: number of weak learners to be considered\n",
    "print('Grid search to find optimal parameters')\n",
    "ar_params = {'n_estimators':np.arange(10,200,10)}\n",
    "gs_ar = GridSearchCV(...) # TO COMPLETE\n",
    "gs_ar.fit(X_train,y_train)\n",
    "c = ... # TO COMPLETE\n",
    "print('- Best number of weak learners (trees) = ',...,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model with best params\n",
    "ab_dt = AdaBoostRegressor(...) # TO COMPLETE\n",
    "model = ab_dt.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the performance on the test set\n",
    "R2 = ab_dt.score(X_test,y_test)\n",
    "print('R2  (test):',R2)\n",
    "mse = mean_squared_error(y_pred,y_test)\n",
    "print('MSE (test):',mse) \n",
    "plt.title('Correlation between actual and predicted values')\n",
    "plt.xlabel('actual values')\n",
    "plt.ylabel('predicted values')\n",
    "plt.scatter(y_test,y_pred,color='royalblue')\n",
    "plt.plot([0,1],[0,1],color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WRITE YOUR ANSWERE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions and complements\n",
    "\n",
    "Here is a list of extensions and complements which can be addressed in the final project:\n",
    "- do complexity pruning for trees by adapting https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html\n",
    "- use XGBoost https://xgboost.readthedocs.io/en/stable/python/python_intro.html, as discussed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
