{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1mrLmaEbiRD"
   },
   "source": [
    "# Classification using Neural Networks (from scratch)\n",
    "\n",
    "This notebook presents an implementation of feedforward neural networks and their training \"from scratch\", in order to understand all the steps involved in the process. The material is adapted from https://nbviewer.org/github/gpeyre/numerical-tours/blob/master/python/ml_6_nn.ipynb \n",
    "At this stage, we do not add any form of regularization (early stopping in the minimization, dropout, weight decay, etc.) and do not separate the train set into an actual training set and a validation set. This will be done in the next notebook, when we actually use neural networks to classify the MNIST digits.\n",
    "\n",
    "**There are 8 questions to answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJc27oW3biRE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set up the random number generator: given seed for reproducibility\n",
    "my_seed = 1\n",
    "np.random.seed(my_seed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9gFuO99biRE"
   },
   "source": [
    "## Creation of the dataset\n",
    "\n",
    "We consider a binary classification problem with output space $\\mathcal{Y} = \\{-1,1\\}$. We work with a synthetic dataset in dimension $d_0=2$, obtained from sampling on half circles with random radii, one of the circles being reflected around the horizontal axis and moved down, in order for the two subpopulations to be closer to one another. \n",
    "\n",
    "The datapoints are returned in the format of matrix, with two lines and as many columns as datapoints. This corresponds to stacking the elements of the dataset (two dimensional column vectors) one after the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TnShS8q8cIoH"
   },
   "outputs": [],
   "source": [
    "noise_level = 0.1\n",
    "\n",
    "def rescale(u,a,b,a1,b1): \n",
    "    return a1 + (u-a)*(b1-a1)/(b-a)\n",
    "\n",
    "def gen_point(m):\n",
    "  y = np.sign(np.random.randn(1,m)) \n",
    "  x = np.zeros((2,m))\n",
    "  r = 1 + noise_level*np.random.rand(m) # random radius, uniformly drawn between 1 and 1.1\n",
    "  t = np.pi/2 + np.pi*np.random.rand(m) # random angle between pi/2 and 3*pi/2\n",
    "  x[0,:] = r * np.sin(t)\n",
    "  x[1,:] = r * np.cos(t)\n",
    "  # reflect and shift for points with negative values of y\n",
    "  I = (y.flatten()<0) \n",
    "  x[0,I] = x[0,I] + 1\n",
    "  x[1,I] = -.7-x[1,I] \n",
    "  # rescale within a box of prescribed size \n",
    "  x[0,:] = rescale(x[0,:],-1,2, .2,.8)\n",
    "  x[1,:] = rescale(x[1,:],-1,.6,.2,.8) \n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sR1hg8jXfyzW"
   },
   "source": [
    "We next display the data points $(x_i,y_i)_{1 \\leqslant i \\leqslant n}$ using a scatter plot for $x^i \\in \\mathbb{R}^2$, with a color depending on the value of $y_i$. We also generate a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "O8wFIDDidnBH",
    "outputId": "183693c1-b1be-4626-d72f-d27195e15c04"
   },
   "outputs": [],
   "source": [
    "# number of points\n",
    "n = 200\n",
    "# training set\n",
    "x, y = gen_point(n)\n",
    "# test set\n",
    "x_test, y_test = gen_point(n)\n",
    "\n",
    "# plotting the training set\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot( x[0,y.flatten()>0], x[1,y.flatten()>0], 'b.',label='y= 1')\n",
    "plt.plot( x[0,y.flatten()<0], x[1,y.flatten()<0], 'r.',label='y=-1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NoMGPx-biRE"
   },
   "source": [
    "## Defining the loss function\n",
    "\n",
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "We consider a network $\\phi_\\theta$ composed of $L-1$ hidden layers of dimensions $(d_1,\\dots,d_{L-1})$, with an input of dimension $d_0$ and an output of dimension $d_L$. It operates by initializing $x_0=x$ and iterating\n",
    "$$ \n",
    "\\forall \\ell=1,\\ldots,L, \\qquad a_{\\ell} := \\rho(W_\\ell a_{\\ell-1} + b_\\ell).  \n",
    "$$\n",
    "The function $\\rho : \\RR \\to \\RR$ is a nonlinear activation function which operates componentwise (and could be different from one layer to the other). The intermediate variables are $a_\\ell \\in \\RR^{d_\\ell}$ with $d_0 = 2$ (the dimension of the input) and $d_L=1$ (the dimension of the ouput). The matrices have sizes $W_\\ell \\in \\RR^{d_{\\ell} \\times d_{\\ell-1}}$, while the biases have sizes $b_\\ell \\in \\RR^{d_{\\ell}}$. The parameters of the networks are $\\theta := (W_\\ell,b_\\ell)_{1 \\leqslant \\ell \\leqslant L}$.\n",
    "\n",
    "The predicted value $x_L = \\phi_\\theta(x)$ is used to compute the parameter $p(x)$ of a Bernoulli distribution using the sigmoid function, as\n",
    "$$\n",
    "p(x) = \\sigma(\\phi_\\theta(x)) = \\frac{\\exp(\\phi_\\theta(x))}{1+\\exp(\\phi_\\theta(x))}.\n",
    "$$\n",
    "The probability that the output $y$ associated with $x$ is 1 is $p(x)$, while the probability that it is -1 is $1-p(x)$. The likelikood of observing $y$ given $x$ is therefore\n",
    "$$\n",
    "\\displaystyle P_\\theta(y|x) = \\frac{1}{1+\\exp(-y \\phi_\\theta(x))}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** *Show that the maximization of the log-likelihood amounts to minimizing the empirical loss:\n",
    "$$\n",
    "\\min_{\\theta} \\frac{1}{n} \\sum_{i=1}^n \\ell(y_i,\\phi_\\theta(x_i)), \\qquad \\ell(z,y) = \\log( 1 + \\exp(-y z) ).\n",
    "$$\n",
    "Compute also the partial derivative $\\partial_z \\ell(z,y)$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Complete the code below to implement the loss function and its derivative. Note that in practice the loss function and its derivative are computed in parallel over a vector of labels $y \\in \\RR^{1 \\times n}$, and an array $X$ of size $(2,n)$ of $n$ points in $\\RR^2$ to which one will apply the neural network in order to obtain a vector of predicted labels $z = \\phi_\\theta(x) \\in \\RR^{1 \\times n}$. The arguments $y,z$ in the code below are therefore vectors of sizes $1 \\times n$, and the gradient to be computed should also be a vector of size $1 \\times n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_JxczY6biRF"
   },
   "outputs": [],
   "source": [
    "# recall that the * product between two vectors corresponds to a componentwise product\n",
    "def Loss(y,z): \n",
    "    return 1/y.shape[1] * np.sum( np.log( 1 + np.exp(-y*z) ) )\n",
    "\n",
    "def nablaLoss(y,z): \n",
    "    return ... ## TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FL5Es4ObiRF"
   },
   "source": [
    "## Building the Network\n",
    "\n",
    "We first define the activation function we use; here an atan sigmoid function. Results would not change much with other activation functions.\n",
    "\n",
    "**Question 3.** Complete the second function, which computes the derivative of the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDxpFasvbiRF"
   },
   "outputs": [],
   "source": [
    "def rho(u): \n",
    "    return np.arctan(u)\n",
    "\n",
    "def rhoG(u): \n",
    "    return ... ## TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGYlrd4ObiRF"
   },
   "source": [
    "The activation function can be displayed in order to gain some intuition on its typical scale of variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "BFZolkhebiRF",
    "outputId": "58cb0b4d-4647-496a-ea42-9eb7e7d68b68"
   },
   "outputs": [],
   "source": [
    "t = np.linspace(-5,5,201)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(t, rho(t))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i43jhxQsbiRF"
   },
   "source": [
    "We can now initialize the topology of the network, *i.e.* provide an array with the dimensions of the various layers. We start by considering a single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvnI3s5sbiRF"
   },
   "outputs": [],
   "source": [
    "dim_hidden_layer = 5\n",
    "D = np.array([2,dim_hidden_layer,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVotsA-hbiRF"
   },
   "source": [
    "All weights and biases are randomly initialized with standard Gaussians (*i.e.* we do not renormalize their scale depending on the number of incoming and outgoing connections). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50my4mJhbiRF"
   },
   "outputs": [],
   "source": [
    "L = D.size-1 \n",
    "W = [] # list of weights\n",
    "b = [] # list of biases\n",
    "for l in np.arange(0,L):\n",
    "    W.append(np.random.randn(D[l+1],D[l]))\n",
    "    b.append(np.random.randn(D[l+1],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7cwv1erbiRF"
   },
   "source": [
    "We next define the function which evaluates the network. There is some bookkeeping for the intermediate results, which is crucial for the subsequent computation of the gradient. The final output is the last element of the list $a$, which can be accessed as $a[-1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_sbSCJ3sbiRF"
   },
   "outputs": [],
   "source": [
    "# format of input x = vector with 2 lines and n columns, i.e. the data points are stacked one after the other \n",
    "def ForwardNN(W,b,x,L):\n",
    "    a = []\n",
    "    a.append(x)\n",
    "    for l in np.arange(0,L):\n",
    "        # matrix multiplication is performed with 'matmul', for which @ is a shorthand notation\n",
    "        # see https://numpy.org/doc/stable/reference/generated/numpy.matmul.html\n",
    "        # the operation performs W[l]*a[l,i]+b[l] for each column i, i.e. for each element of the dataset\n",
    "        # the np.tile function creates a matrix where the column b[l] is repeated \n",
    "        # as many times as there are datapoints\n",
    "        # note that the convention is slighly different from the one seen in the lectures \n",
    "        # since a starts from the index 0, while W,b start from index 1 in our theoretical presentation\n",
    "        # while they start from index 0 in the actual computer implementation\n",
    "        a.append( rho( W[l]@a[l] + np.tile(b[l],[1,x.shape[1]]) ) )\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the evaluation of the network and the loss in order to make sure that everything is well defined at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nnsc5XVlmwqT"
   },
   "outputs": [],
   "source": [
    "a = ForwardNN(W,b,x,L)\n",
    "Ls = Loss(a[-1],y)\n",
    "print(\"Value of the loss function: \",Ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krzXfqI8biRF"
   },
   "source": [
    "## Network Optimization\n",
    "\n",
    "We optimize the network parameters by minimizing the non-convex empirical loss function with a simple stochastic gradient descent with fixed stepsize. This requires computing the gradient with respect to $\\theta$ of the terms $f_i(\\theta) = \\ell(y_i,\\phi_\\theta(x_i))$ in the empirical loss. This is done for every data point, the total gradient being then obtained by a linear combination of the individual gradients associated with a specific data point.\n",
    "\n",
    "The gradients of $(f_i)_{1 \\leqslant i \\leqslant n}$ are initialized by computing the derivatives with respect to the final output $a_L$ as\n",
    "$$ \n",
    "g^L := \\partial_z \\ell(y,a_L) = \\left( \\partial_z \\ell(y_i,a_{i,L}) \\right)_{1 \\leqslant i \\leqslant n} \\in \\RR^{1 \\times n}\n",
    "$$ \n",
    "where $\\partial_z \\ell$ is applied componentwise to $a_L$ and $y$. Since the loss function is a scalar, we obtain a scalar number for every data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZl0wON3biRF"
   },
   "outputs": [],
   "source": [
    "gx = nablaLoss(a[L],y)\n",
    "print(\"Shape of gradient with respect to output:\",gx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hP0oIknobiRF"
   },
   "source": [
    "The successive gradients with respect to the intermediate variables $a_\\ell$ are solutions of a backward recursion, corresponding to the celebrated backpropagation algorithm. See for instance http://neuralnetworksanddeeplearning.com/chap2.html for a presentation. Concretely, introducing the weighted inputs $z_\\ell = W_\\ell a_{\\ell-1} + b_\\ell$ (beware that, in the code, there is an index shift on $a$), one defines \n",
    "$$ \n",
    "\\forall \\ell=L,\\ldots,1, \\qquad \\delta^\\ell := \\nabla_{z_\\ell} f_i, \\qquad g^\\ell := \\nabla_{a_{\\ell}} f_i. \n",
    "$$\n",
    "Then, the following recursion relations hold starting from $g^L$ (which corresponds to the variable *gx* in the argument of the function *BackwardNN* below):\n",
    "$$\n",
    "\\delta^{\\ell} = g^{\\ell} \\odot \\rho'(z_{\\ell}), \n",
    "\\qquad\n",
    "g^{\\ell-1} = W_\\ell^T \\delta^\\ell.\n",
    "$$\n",
    "where $\\odot$ denotes entry-wise multiplication. From these gradients with respect to the intermediate layers variables, the gradient with respect to the network parameters are retrieved as\n",
    "$$ \n",
    "\\nabla_{W_\\ell} f = \\delta^\\ell a_{\\ell-1}^\\top, \\qquad \\nabla_{b_\\ell} f_i = \\delta^\\ell.  \n",
    "$$\n",
    "\n",
    "**Question 4.** Complete the code below to conclude the implementation of backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wml50goEbiRF"
   },
   "outputs": [],
   "source": [
    "def BackwardNN(W,b,a,L,gx):\n",
    "    gW = [] # gradient with respect to W\n",
    "    gb = [] # gradient with respect to b\n",
    "    n = a[0].shape[1] # number of samples in the mini-batch\n",
    "    for l in np.arange(0,L):\n",
    "        gW.append([]) \n",
    "        gb.append([])\n",
    "    for l in np.arange(L-1,-1,-1):\n",
    "        # compute the matrix \\delta^l, which stacks the gradients for each data point one after the other\n",
    "        # shape = d_{l} lines, n columns\n",
    "        delta = rhoG( (W[l] @ a[l]) + np.tile(b[l],[1,n]) ) * gx\n",
    "        # nabla_a[l]; stacking n columns of vectors of size d_{l-1} \n",
    "        gx = ... ## TO COMPLETE\n",
    "        # nabla_W[l]; using a[l].T allows to sum over the data points\n",
    "        # the resulting gradient is the transpose of a matrix of size d_{l} x d_{l-1}\n",
    "        gW[l] =  ... ## TO COMPLETE\n",
    "        # nabla_b[l]: need to sum over the data points in order to obtain a column vector of size d_{l}\n",
    "        # we then restransform into a column vector; -1 in reshape means \"whatever it takes to flatten\"\n",
    "        gb[l] =  (delta.sum(axis=1)).reshape(-1,1) \n",
    "    return [gW,gb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjzZ9eF_rx2P"
   },
   "source": [
    "We now implement the computation of $\\nabla_\\theta f = (\\nabla_W f,\\nabla_b f)$ by performing first the forward pass and then the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bs3JJtIOxSnN"
   },
   "outputs": [],
   "source": [
    "def ForwardBackwardNN(W,b,x,y):\n",
    "    # forward pass\n",
    "    a = ForwardNN(W,b,x,L)\n",
    "    # computation of the loss\n",
    "    Ls = Loss(y,a[L])\n",
    "    # initialization of the gradient\n",
    "    gx = nablaLoss(y,a[L])\n",
    "    # backward pass\n",
    "    [gW,gb] = BackwardNN(W,b,a,L,gx)\n",
    "    return [Ls,gW,gb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hc3CffOQtO5x"
   },
   "source": [
    "The weights and biases of the various layers are randomly initialized according to standard Gaussian random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ow9MfGxptOHd"
   },
   "outputs": [],
   "source": [
    "for l in np.arange(0,L):\n",
    "    W[l] = np.random.randn(D[l+1],D[l])\n",
    "    b[l] = np.random.randn(D[l+1],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uh6K-7cxtdIK"
   },
   "source": [
    "We use here a simple gradient dynamics, without minibatching. To run the dynamics, we need to select the step size $\\tau$. It should be roughly independent of the batch size $n$ (since the loss function is normalized by a factor $1/n$), but depends on the size and number of layers. You should try different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "zTiz4F6HbiRF",
    "outputId": "c93c8da7-4b62-4db3-862e-61ba1333ca2f"
   },
   "outputs": [],
   "source": [
    "# parameters for the optimization\n",
    "tau = ... ## TO COMPLETE\n",
    "niter = ... ## TO COMPLETE\n",
    "\n",
    "# keep track of the values of the loss function \n",
    "Ls = np.zeros((niter,1))\n",
    "Ls_test = np.zeros((niter,1))\n",
    "for it in np.arange(0,niter):\n",
    "    # save the value of the loss function at the beginning of each epoch\n",
    "    [Ls[it],gW,gb] = ForwardBackwardNN(W,b,x,y)\n",
    "    # compute the loss of the test set\n",
    "    a = ForwardNN(W,b,x_test,L)\n",
    "    Ls_test[it] = Loss(y_test,a[-1])\n",
    "    # update all parameters using the gradient which was computed\n",
    "    for l in np.arange(0,L):\n",
    "        W[l] = W[l] - tau*gW[l]\n",
    "        b[l] = b[l] - tau*gb[l]\n",
    "        \n",
    "# plot the results\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(Ls,color='red',label='train')\n",
    "plt.plot(Ls_test,color='blue',label='test')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('value of the loss function')\n",
    "plt.axis('tight')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iTtwIzcbiRF"
   },
   "source": [
    "We next evaluate the quality of the so-obtained network for classification. We generate to this end a test set of point $z_i \\in \\RR^2$ on a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZysaMm7pbiRF"
   },
   "outputs": [],
   "source": [
    "nb_points_grid = 100\n",
    "t = np.linspace(0,1,nb_points_grid)\n",
    "# a mesh is conveniently generated using the function 'meshgrid'\n",
    "# see https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html\n",
    "[U,V] = np.meshgrid(t,t)\n",
    "# the so-obtained x and y values of the points composing the 2D grid are then stacked as a column vector\n",
    "# in order to evalue the function at these points\n",
    "z = np.vstack([V.flatten(), U.flatten()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijpkKv0rbiRF"
   },
   "source": [
    "We compute the classification \"probability\" at each location $z$, obtained from the formula $\\displaystyle \\sigma(\\phi_\\theta(x)) = \\frac{\\exp(\\phi_\\theta(x))}{1+\\exp(\\phi_\\theta(x))}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9YRXQrQbiRF"
   },
   "outputs": [],
   "source": [
    "def phi(r): \n",
    "    return np.exp(r)/(1+np.exp(r))\n",
    "\n",
    "# evaluate the neural network at the points of the grid (stacked vertically)\n",
    "V = ForwardNN(W,b,z,L)\n",
    "# reshape the so-obtained function to plot it\n",
    "U = np.reshape( phi(V[L].T), [nb_points_grid,nb_points_grid] )\n",
    "\n",
    "# plot of the probabilities estimated with the neural network\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(U.T, origin=\"lower\", extent=[0,1,0,1])\n",
    "plt.plot( x[0,y.flatten()>0], x[1,y.flatten()>0], 'b.' )\n",
    "plt.plot( x[0,y.flatten()<0], x[1,y.flatten()<0], 'r.' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0scKo3pbiRG"
   },
   "source": [
    "**Question 5.** Test the performance of the final network as a function of the number of training steps. Vary also the number of layers and check the influence of the topology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDuNq5HvbiRG"
   },
   "source": [
    "# Automatic differentiation\n",
    "\n",
    "Instead of computing the gradient \"by hand\", we use an automatic differentation toolbox, such as [AutoGrad](https://github.com/HIPS/autograd). It uses reverse mode automatic differentation to compute the gradient at the same expense at computing the function itself. \n",
    "See [this tutorial for more information](https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation).\n",
    "\n",
    "Autograd is no longer actively maintained, the automatic differentiation tool of JAX being nowadays the reference; see https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwrmVLMCbiRG"
   },
   "outputs": [],
   "source": [
    "# using autograd\n",
    "import autograd.numpy as np\n",
    "import autograd as ag\n",
    "from autograd import elementwise_grad as egrad # for functions that vectorize over inputs; grad otherwise\n",
    "\n",
    "# using JAX\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L92f6gsXbiRG"
   },
   "source": [
    "We test the automatic differentiation on a scalar valued function of a single variable, by computing and plotting higher-order derivatives.\n",
    "\n",
    "**Question 6.** Complete the code below to cross check automatic differentiation with a finite difference approximation for the first and second derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "wCF3VhuHbiRG",
    "outputId": "93402593-e2b0-4060-da28-11b9c1e5aca1"
   },
   "outputs": [],
   "source": [
    "# define function for numpy\n",
    "def tanh(x):\n",
    "    return (1.0 - np.exp(-x))  / (1.0 + np.exp(-x))\n",
    "\n",
    "# define function for numpy in JAX\n",
    "def jtanh(x):\n",
    "    return (1.0 - jnp.exp(-x))  / (1.0 + jnp.exp(-x))\n",
    "\n",
    "# range for the function\n",
    "t = np.linspace(-7, 7, 200)\n",
    "# to compare with finite difference approximation\n",
    "eps = 10**(-6)\n",
    "\n",
    "fig = plt.figure(figsize=(16,6))\n",
    "# with Autograd\n",
    "ax0 = fig.add_subplot(1, 2, 1)\n",
    "ax0.plot(t, tanh(t),label='order 0')\n",
    "ax0.plot(t, egrad(tanh)(t),label='order 1')                                     # first derivative\n",
    "ax0.plot(t, egrad(egrad(tanh))(t),label='order 2')                              # second derivative\n",
    "ax0.plot(t, egrad(egrad(egrad(tanh)))(t),label='order 3')                       # third derivative\n",
    "ax0.plot(t, egrad(egrad(egrad(egrad(tanh))))(t),label='order 4')                # fourth derivative\n",
    "ax0.plot(t, egrad(egrad(egrad(egrad(egrad(tanh)))))(t),label='order 5')         # fifth derivative\n",
    "ax0.plot(t, egrad(egrad(egrad(egrad(egrad(egrad(tanh))))))(t),label='order 6')  # sixth derivative\n",
    "ax0.legend()\n",
    "# with JAX + comparison with finite differences\n",
    "ax1 = fig.add_subplot(1, 2, 2)\n",
    "ax1.plot(t, tanh(t),label='order 0')\n",
    "# vmap used to vectorize operations\n",
    "# see https://www.kaggle.com/code/aakashnain/tf-jax-tutorials-part-9-autodiff-in-jax#Gradients\n",
    "d1_tanh = grad(jtanh)\n",
    "d1_values = vmap(d1_tanh)(t)\n",
    "ax1.plot(t, d1_values,label='order 1')\n",
    "ax1.plot(t,...,label='order 1 (finite diff)') ## TO COMPLETE\n",
    "d2_tanh = grad(d1_tanh)\n",
    "d2_values = vmap(d2_tanh)(t)\n",
    "ax1.plot(t, d2_values,label='order 2')\n",
    "ax1.plot(t,...,label='order 2 (finite diff)') ## TO COMPLETE\n",
    "ax1.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8CYqtNObiRG"
   },
   "source": [
    "We can next consider functions of several variables, for instance the quadratic function $\\displaystyle f(u)=\\frac12 u^\\top M u$ for $M$ symmetric, whose gradient is $\\nabla f(u)=Mu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_BbQzr83biRG",
    "outputId": "163b9fc6-f788-47a5-9fde-90d0ca97f06c"
   },
   "outputs": [],
   "source": [
    "# construct a random symmetric matrix \n",
    "M = np.random.randn(3,3)\n",
    "M = M+M.T\n",
    "\n",
    "# quadratic function\n",
    "def f(u): \n",
    "    return np.sum( .5 * u.T @ (M@u) )\n",
    "\n",
    "# evaluation of the gradient at a random point; simply 'grad' here since no vectorization\n",
    "u = np.random.randn(3,1)\n",
    "g = ag.grad(f)\n",
    "print('Difference between analytic gradient and autograd: ' + str( np.linalg.norm(np.abs((g(u) - M@u) ) )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUd7s3gebiRG"
   },
   "source": [
    "We next consider the loss function $f(\\theta)$ to be minimized for classification using neural networks, with $\\theta=(W,b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lckQ7H980tPo"
   },
   "outputs": [],
   "source": [
    "def FuncNN(theta): \n",
    "    a = ForwardNN(theta[0],theta[1],x,L)\n",
    "    return Loss(a[-1],y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JgdIhO7ibiRG"
   },
   "outputs": [],
   "source": [
    "D = [2, 4, 5, 8, 1]\n",
    "L = np.size(D)-1\n",
    "W = []\n",
    "b = []\n",
    "for l in np.arange(0,L):\n",
    "    W.append(np.random.randn(D[l+1],D[l]))\n",
    "    b.append(np.random.randn(D[l+1],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tl41-PzEbiRG"
   },
   "source": [
    "We compute the function and its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ga39gevRbiRG"
   },
   "outputs": [],
   "source": [
    "FuncNNG = ag.value_and_grad(FuncNN)\n",
    "tht = (W,b)\n",
    "[u,g] = FuncNNG(tht)\n",
    "gW = g[0] # gradient with respect to A \n",
    "gb = g[1] # gradient with respect to b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00ramBszbiRG"
   },
   "source": [
    "**Question 7.** Compare the gradient computed by automatic differentiation and the one computed \"by hand\" using backpropagation (show for instance that the norm of the difference is of the order of machine precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4CwiUDhrbiRG",
    "outputId": "b4062737-83d1-4a19-b626-aa98a51e95eb"
   },
   "outputs": [],
   "source": [
    "# TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-rwPX7k2PJT"
   },
   "source": [
    "# Automatic differentiation using *PyTorch*\n",
    "\n",
    "Pytorch is a powerful deep learning library whose main feature is to perform automatic differentiation. One caveat is that one need to manipulate PyTorch tensors in place of Numpy arrays. An avantage with respect to AutoGrad is that it contains higher level primitive which allows to hide from the user the parameter of the layer (e.g. linear weights and bias). See https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html# for an introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_23Hnrdy2Q0d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGXxLWjBAXX_"
   },
   "source": [
    "We first define the architecture. You can try `nn.ReLu()` in place of `nn.Tanh()` to obtain straight boundaries. One can try one of the two following cell depending of the number of hidden layers you want (there are more automatic ways to code this up but we keep a simple version here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fx3FA-4toiwZ"
   },
   "outputs": [],
   "source": [
    "D = [2, 4, 5, 8, 4, 1]\n",
    "model = nn.Sequential(\n",
    "      nn.Linear(D[0], D[1]),\n",
    "      nn.Tanh(),\n",
    "      nn.Linear(D[1], D[2]),\n",
    "      nn.Tanh(),\n",
    "      nn.Linear(D[2], D[3]),\n",
    "      nn.Tanh(),\n",
    "      # could add dropout somewhere with an additional line like: nn.Dropout(0.1),\n",
    "      nn.Linear(D[3], D[4]),\n",
    "      nn.Tanh(),\n",
    "      nn.Linear(D[4], D[5]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4VOk3OSbAJM"
   },
   "outputs": [],
   "source": [
    "D = [2, 20, 1]\n",
    "model = nn.Sequential(\n",
    "      nn.Linear(D[0], D[1]),\n",
    "      nn.Tanh(),\n",
    "      nn.Linear(D[1], D[2]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LEgLb7v4_Hp_",
    "outputId": "334c4783-63dc-4d44-d794-b8b26747790a"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPoYuBxDo1yD"
   },
   "source": [
    "We first initialize the weights with  Gaussian random variables using the dedicated PyTorch routine https://pytorch.org/docs/stable/nn.init.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeUiaYgDcs10"
   },
   "outputs": [],
   "source": [
    "L = np.size(D)-1\n",
    "for l in range(0,L):\n",
    "    print('Initializing model[',2*l,']')\n",
    "    nn.init.normal_(model[2*l].bias, 0, 1)\n",
    "    nn.init.normal_(model[2*l].weight, 0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaH6M9zyBZPV"
   },
   "source": [
    "We next convert arrays into PyTorch tensors. See here for a more complete discussion of these classes https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html Beware that for PyTorch the data should be of size $(n,2)$ so that we apply the function *model* to it, which is why we need to transpose the tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AsRfvXA6BIxX"
   },
   "outputs": [],
   "source": [
    "X = torch.Tensor(x).T\n",
    "Y = torch.Tensor(y).T\n",
    "X_test = torch.Tensor(x_test).T\n",
    "Y_test = torch.Tensor(y_test).T\n",
    "print(\"Data shape:\",X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTJKwlvLGVzn"
   },
   "source": [
    "We first run the forward pass and evaluate the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "guCONbyAEqT4",
    "outputId": "72044c61-f530-4aa2-b3d7-47a989f9f89f"
   },
   "outputs": [],
   "source": [
    "loss = 1/n * torch.sum( -torch.log( torch.sigmoid(-model(X)*Y) ) )\n",
    "# printing the loss: tensor of a single element, hence using .item() to access this element\n",
    "print( loss.item() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Rbi2XnxGclF"
   },
   "source": [
    "We next run the backward pass to evaluate the gradients of all the parameters involved in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dC4v28ahGar-"
   },
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BnDZh6YGt5o"
   },
   "source": [
    "We can then perform one step of the gradient method to update each parameter as $\\theta \\leftarrow \\theta - \\tau \\nabla f(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3p0cfDC9GpLi"
   },
   "outputs": [],
   "source": [
    "tau = ... ## TO COMPLETE\n",
    "# use no_grad in order to update parameters\n",
    "# this way, they keep the \"requires_grad=True\" flag, which is fundamental to automatically compute the gradient\n",
    "# you can compare what happens when changing the update rule to \"theta -= tau * theta.grad\"\n",
    "# or when torch.no_grad() is commented out\n",
    "# in both cases, tensor(..., requires_grad=True) is transformed into something different\n",
    "# e.g. tensor(...) or tensor(..., grad_fn=<SubBackward0>)\n",
    "with torch.no_grad():\n",
    "    for theta in model.parameters():\n",
    "        # to see the type before: print(theta)\n",
    "        theta = theta - tau * theta.grad\n",
    "        # to see the type after: print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4H5FpcnGi6v"
   },
   "source": [
    "Gradient descent can finally be obtained by putting all these elementary steps together. We do not do any form of regularization here, even early stopping. This will be needed in actual applications (see the next notebook).\n",
    "\n",
    "**Question 8.** Compare the results obtained with various parameters of the model (in particular architecture and activation functions) and of the optimization function. Note that this comparison cannot be too quantitative since we would need to implement early stopping in order to stop optimization when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92Bg21HqAdqA"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "tau = ... # TO COMPLETE\n",
    "niter = ... # TO COMPLETE\n",
    "\n",
    "# initialization of the vector of values of the loss functions\n",
    "Ls = np.zeros((niter,1))\n",
    "Ls_test = np.zeros((niter,1))\n",
    "\n",
    "# iterations over the epochs\n",
    "for i in range(niter):\n",
    "    loss = 1/n * torch.sum( -torch.log( torch.sigmoid(model(X)*Y) ) )\n",
    "    Ls[i] = loss.item()\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        loss = 1/n * torch.sum( -torch.log( torch.sigmoid(model(X_test)*Y_test) ) )\n",
    "        Ls_test[i] = loss.item()\n",
    "        for theta in model.parameters():\n",
    "            theta -= tau * theta.grad\n",
    "   \n",
    "# plotting the results\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(L,color='red',label='train')\n",
    "plt.plot(L_test,color='blue',label='test')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('value of the loss function')\n",
    "plt.axis('tight')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLicOTqcV6HB"
   },
   "source": [
    "We can finally vizualize the application of the network on the grid points defined by `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "I8amc71RVXs6",
    "outputId": "e56839ae-5362-4edd-cf19-594a279e5054"
   },
   "outputs": [],
   "source": [
    "# evaluate the neural network at the grid points \n",
    "V = model(torch.Tensor(z).T)\n",
    "# convert back to numpy array\n",
    "V = V.detach().numpy() \n",
    "# reshape the so-obtained function to plot it\n",
    "U = np.reshape( phi(V), [nb_points_grid,nb_points_grid] )\n",
    "\n",
    "# plot of the probabilities estimated with the neural network\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(U.T, origin=\"lower\", extent=[0,1,0,1])\n",
    "plt.plot( x[0,y.flatten()>0], x[1,y.flatten()>0], 'b.' )\n",
    "plt.plot( x[0,y.flatten()<0], x[1,y.flatten()<0], 'r.' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Neural Networks and AutoDiff",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
