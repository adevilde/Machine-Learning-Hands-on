{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of clustering methods\n",
    "\n",
    "We compare in this final hands-on various clustering methods, on toy data sets (inspired by https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html). See https://scikit-learn.org/stable/modules/clustering.html# for an introduction to clustering methods, and in particular https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation for elements on how to evaluate the performance of clustering methods. We focus here on the silhouette coefficient since this is the first metric in the list which can be used when labels are not known. Other metrics could of course be considered.\n",
    "\n",
    "We focus on $K$-means, agglomerative clustering, density-based methods (DBSCAN) and Gaussian mixture. Spectral methods can also be tested.\n",
    "\n",
    "**There are 6 questions to answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from sklearn import cluster, datasets\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** What is the silhouette score? Which values of this metric are the best? The worst?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation\n",
    "\n",
    "We first generate datasets. Their sizes are large enough to see the scalability of the algorithms, but not too large to avoid too long running times. See https://scikit-learn.org/stable/datasets/sample_generators.html#sample-generators for a more precise description of the methods to generate datasets, and the underlying rationale behind them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desired number of samples\n",
    "n_samples = 1500\n",
    "\n",
    "# reinitialization of the random number generator\n",
    "np.random.seed(0)\n",
    "\n",
    "#--- generation of four target datasets ----\n",
    "# see https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,noise=.05)\n",
    "# see https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "# see https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "# unstructured dataset = uniform distribution, no label for coloring\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "chosen_datasets = [noisy_circles, noisy_moons, blobs, no_structure]\n",
    "chosen_datasets_names = ['noisy_circles', 'noisy_moons', 'blobs', 'no_structure']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next plot the datasets. They could also be normalized to study the impact of renormalization procedures on the quality of the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "# baseline colors\n",
    "colors = np.array([x for x in 'brkgmcy'])\n",
    "# repeating the color vector for situations when there are many clusters\n",
    "colors = np.hstack([colors] * 20)\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    Y = chosen_datasets[i][1]\n",
    "    X = chosen_datasets[i][0]\n",
    "    # if normalization is wanted: add the following two lines \n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    chosen_datasets[i] = X,Y\n",
    "    if Y is None:\n",
    "        plt.scatter(X[:,0],X[:,1],s=10,color=colors[0])\n",
    "    else:\n",
    "        plt.scatter(X[:,0],X[:,1],s=10,color=colors[Y])\n",
    "    plt.title(chosen_datasets_names[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first invidually test all clustering methods on the various datasets, in order to fine tune their parameters.\n",
    "\n",
    "# $K$-means\n",
    "\n",
    "The important parameters to choose for $K$-means are the number of classes, and the way distances are measured. See https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choice of dataset\n",
    "index = 2\n",
    "# choice of parameters for K-means\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=10, algorithm='lloyd')\n",
    "\n",
    "# performing the clustering\n",
    "X,Y = chosen_datasets[index]\n",
    "Y_pred = kmeans.fit_predict(X)\n",
    "\n",
    "# plotting the classes and the cluster centers\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(X[:,0],X[:,1],s=10,color=colors[Y_pred])\n",
    "plt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],marker='o',s=100,color='y')\n",
    "plt.show()\n",
    "\n",
    "# evaluation of the performance\n",
    "print(\"Silhouette score:\",silhouette_score(X,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Test the influence of the various parameters and the nature of the dataset on the results. Start with the 'blobs' data set and $K=3$ clusters, then change the number of clusters; next change the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative clustering\n",
    "\n",
    "One drawback of $K$-means is that the number of clusters has to be fixed in advance. This can be avoided by progressively merging data clusters, and inspecting the associated graph of overall loss. See https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function to plot the dendogram, see https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html. It creates the linkage matrix first, then calls the dedicated scipy function https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html#scipy.cluster.hierarchy.dendrogram\n",
    "The various elements are based on the attributes of the agglomerative clustering method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "    # create the linkage matrix with the appropriate format\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_, counts]).astype(float)\n",
    "    # plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choice of dataset\n",
    "index = 0\n",
    "# setting distance_threshold=0 ensures that the full tree is computed\n",
    "agglo_cluster = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
    "\n",
    "# performing the clustering\n",
    "X,Y = chosen_datasets[index]\n",
    "agglo_cluster.fit(X)\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plot_dendrogram(agglo_cluster, truncate_mode=\"level\", p=5) # change depth of representation with p\n",
    "plt.ylim(-0.2,6) # to better see the beginning\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can next decide on a number of clusters and perform the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering with a fixed number of clusters\n",
    "agglo_cluster = AgglomerativeClustering(n_clusters=3)\n",
    "X,Y = chosen_datasets[index]\n",
    "agglo_cluster.fit(X)\n",
    "Y_pred = agglo_cluster.labels_\n",
    "\n",
    "# plot the dataset and the clusters\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(X[:,0],X[:,1],s=10,color=colors[Y_pred])\n",
    "plt.show()\n",
    "\n",
    "# evaluation of the performance\n",
    "print(\"Silhouette score:\",silhouette_score(X,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.** Which distance between clusters is used by default? Test the influence of the various parameters and the nature of the dataset on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density-based methods (DBSCAN)\n",
    "\n",
    "We next consider a density-based method which automatically sets the number of clusters depending on input parameters. See https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html and https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choice of dataset\n",
    "index = 2\n",
    "dbscan_cluster = DBSCAN(eps=0.1, min_samples=5)\n",
    "\n",
    "# performing the clustering\n",
    "X,Y = chosen_datasets[index]\n",
    "dbscan_cluster.fit(X)\n",
    "Y_pred = dbscan_cluster.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(Y_pred)) - (1 if -1 in Y_pred else 0)\n",
    "n_noise_ = list(Y_pred).count(-1)\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "\n",
    "# plot the figure\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(X[:,0],X[:,1],s=10,color=colors[Y_pred])\n",
    "plt.show()\n",
    "\n",
    "# evaluation of the performance\n",
    "print(\"Silhouette score:\",silhouette_score(X,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.** Test the influence of the various parameters for the datasets at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian mixture models\n",
    "\n",
    "We consider here the use of Gaussian mixture models, which can be seen as some soft K-means. See https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture and https://scikit-learn.org/stable/modules/mixture.html#gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choice of dataset\n",
    "index = 2\n",
    "gm_cluster = GaussianMixture(n_components=3, random_state=0)\n",
    "\n",
    "# performing the clustering\n",
    "X,Y = chosen_datasets[index]\n",
    "Y_pred = gm_cluster.fit_predict(X)\n",
    "\n",
    "# plot the figure\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(X[:,0],X[:,1],s=10,color=colors[Y_pred])\n",
    "plt.show()\n",
    "\n",
    "# evaluation of the performance\n",
    "print(\"Silhouette score:\",silhouette_score(X,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.** Test the influence of the various parameters for the datasets at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral methods\n",
    "\n",
    "We finally resort to a spectral method, corresponding to the normalized graph cut. See https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html and https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choice of dataset\n",
    "index = 0\n",
    "spectral_cluster = SpectralClustering(n_clusters=2,n_init=10,assign_labels='kmeans',gamma=10.) \n",
    "#-- note: can try other ways to assign labels, e.g. 'discretize' or 'cluster_qr'\n",
    "\n",
    "# performing the clustering\n",
    "X,Y = chosen_datasets[index]\n",
    "spectral_cluster.fit(X)\n",
    "Y_pred = spectral_cluster.labels_\n",
    "\n",
    "# plot the figure\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(X[:,0],X[:,1],s=10,color=colors[Y_pred])\n",
    "plt.show()\n",
    "\n",
    "# evaluation of the performance\n",
    "print(\"Silhouette score:\",silhouette_score(X,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6.** Test the influence of the various parameters for the datasets at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension: MNIST data\n",
    "\n",
    "For the final project, one possible extension to consider is to test (some of) the above methods for MNIST data, and compare the performance of the clustering methods using the information available on the labels. The values of the parameters should be tested more thoroughly than here.\n",
    "\n",
    "One could also consider generative methods based on GMM; see https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html, as well as https://datahype.net/2015/12/29/mixture-models-intro/ for a very simple code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
